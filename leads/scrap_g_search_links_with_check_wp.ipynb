{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 48,
            "source": [
                "# Scripts looks for WordPress websites in Google Search\r\n",
                "# Creation date: Sept 2022\r\n",
                "# Last modification: 05 Jan 2023\r\n",
                "\r\n",
                "# IDEA: Save non-WordPress sites - maybe I can offer them something too\r\n",
                "# IDEA: Drop .gov and .edu domains - done in \"drop_gov_and_edu.ipynb\" (for emails, but should also be done for domains)\r\n",
                "# IDEA: Maybe the script should append currently exisitng excel so it can be stopped any time\r\n",
                "\r\n",
                "\r\n",
                "from bs4 import BeautifulSoup\r\n",
                "import openpyxl\r\n",
                "import requests, re\r\n",
                "import pandas as pd\r\n",
                "import numpy as np\r\n",
                "from urllib.parse import urlsplit\r\n",
                "from collections import deque\r\n",
                "from datetime import datetime, date\r\n",
                "from time import sleep\r\n",
                "\r\n",
                "search_for = \"car rental\" #Change only this string\r\n",
                "search_for = search_for.replace(\" \", \"+\")\r\n",
                "headers = {\r\n",
                "    \"User-agent\" : \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\"\r\n",
                "}\r\n",
                "cities = ['Clovis']#, 'Seattle', 'Los Angeles', 'Tampa', 'Brooklyn', 'Phoenix','Evansville','Springfield','Independence','Ann Arbor','Hartford','Hillsboro','Abilene']\r\n",
                "# cities = ['Roanoke','Nampa ','Edinburg ','Clinton ','Federal Way ','New Bedford ','Lees Summit ','Lynn ','Chico ','Quincy ','Davenport ','Wichita Falls ',\r\n",
                "# 'Vacaville ','Menifee ','Norwalk ','Spokane Valley ','South Bend ','Woodbridge ','Rialto ','Rio Rancho ','Allen ','Daly City ','Jurupa Valley','Concord ',\r\n",
                "# 'Brockton ','San Mateo ','Davie','Tyler ','El Cajon ','Hillsboro ','Renton ','Burbank ','Green Bay ','South Fulton ','Edison ','Inglewood ','Sandy Springs ',\r\n",
                "# 'Boulder ','Centennial ','Sparks ','Greeley ','El Monte ','West Covina ','Santa Maria ','Dearborn ','Temecula ','Everett ','Ventura','Murrieta ','Sugar Land ',\r\n",
                "# 'Las Cruces ','Miami Gardens ','Lewisville ','Pueblo ','Costa Mesa ','Pompano Beach ','Lakeland ','Lansing ','Peoria ','Broken Arrow ','High Point ','Gresham ',\r\n",
                "# 'Downey ','League City ','Springfield ','Waterbury ','Odessa ','Carlsbad ','Elgin ','North Charleston ','Provo ','Beaumont ','Antioch ','Wilmington ','Lowell ',\r\n",
                "# 'Manchester ','Westminster ','Richmond ','West Jordan ','Billings ','Clearwater ','Evansville ','West Palm Beach ','Meridian ','Cambridge ','Round Rock ','Richardson ',\r\n",
                "# 'Palm Bay ','Fairfield ','Clovis ','College Station ','Hartford ','Lafayette','Rochester ','Independence ','Ann Arbor ','Berkeley ','Arvada ','Abilene ']\r\n",
                "\r\n",
                "def try_except(func, *arg):\r\n",
                "    try:\r\n",
                "        result = func(*arg)\r\n",
                "        print(func.__name__, \":\", *arg, \"=\", result)\r\n",
                "        return result\r\n",
                "    except Exception as e:\r\n",
                "        print(\"1:\", e)\r\n",
                "        return \"N/a\"\r\n",
                "\r\n",
                "def check_if_wordpress(url):\r\n",
                "    r = requests.get(url, headers=headers, timeout=5)\r\n",
                "    if \"wordpress\" in r.text:\r\n",
                "        return True\r\n",
                "    else:\r\n",
                "        return False\r\n",
                "\r\n",
                "# def scrap_href(link):\r\n",
                "#     links = link.a[\"href\"]\r\n",
                "#     links = links.split(\"/\", 3)[:3]\r\n",
                "#     links[1] = \"//\"\r\n",
                "#     links[0:3] = [''.join(links[0:3])]\r\n",
                "#     return links[0]\r\n",
                "       \r\n",
                "#Scrapping URLs\r\n",
                "is_wordpress = []\r\n",
                "links_list = []\r\n",
                "skips_number = 0\r\n",
                "for city in cities:\r\n",
                "    n = 1\r\n",
                "    exeptions_number = 0\r\n",
                "    url = f\"https://www.google.com/search?q={search_for}+{city}\"\r\n",
                "    while n < 10: #number of pages to scrap should be 10\r\n",
                "        progress = ((cities.index(city) * 10 + n) / (len(cities) * 10)) * 100 # Counting progress %\r\n",
                "        print(city, n, \"/ 10 | Progress: \", \"{:.2f}\".format(progress), \"%\")\r\n",
                "        try:\r\n",
                "          if n == 1:\r\n",
                "              response = requests.get(url, headers=headers, timeout=5)\r\n",
                "              soup = BeautifulSoup(response.text, 'lxml')\r\n",
                "          else: # It will be executed when n >= 2\r\n",
                "              next_page_url = soup.find(\"a\", {\"aria-label\":f\"Page {n}\"})[\"href\"]\r\n",
                "              response = requests.get(f\"https://google.com{next_page_url}\", headers=headers)\r\n",
                "              soup = BeautifulSoup(response.text, 'lxml')\r\n",
                "          for link in soup.find_all(\"div\", {\"class\":\"g\"}): # There should be 9-10 divs (results) with this class\r\n",
                "              links = link.a[\"href\"] # Taking only \"a.href\" (links) from div\r\n",
                "              links = links.split(\"/\", 3)[:3] # Taking only link till 3rd \"/\" (https://smtng.com/...)\r\n",
                "              links[1] = \"//\"\r\n",
                "              links[0:3] = [''.join(links[0:3])]\r\n",
                "              links_list.append(links[0]) # Saving link to list\r\n",
                "              is_wordpress.append(try_except(check_if_wordpress, links[0]))\r\n",
                "        except Exception as e:\r\n",
                "            print(\"2:\", e) #If \"NoneType\" error - probably google blocked the IP\r\n",
                "            exeptions_number += 1\r\n",
                "            if exeptions_number > 2: #If 3 errors in one \"city\" scrap then break 'while' loop\r\n",
                "                sleep(10)\r\n",
                "                if n == 1: #If error occures in 1st loop execution (n) then add skips_number\r\n",
                "                    skips_number += 1\r\n",
                "                print(f\"Exeptions_number is: {exeptions_number}. Breaking 'while' loop.\")\r\n",
                "                break\r\n",
                "            else:\r\n",
                "                sleep(2)\r\n",
                "            pass\r\n",
                "        sleep(0.5)\r\n",
                "        n += 1\r\n",
                "    if skips_number > 2: #If 3 errors in 1st loop execution (n) then break 'for' loop\r\n",
                "        print(f\"skips_number is: {skips_number}. Breaking 'for' loop.\")\r\n",
                "        break\r\n",
                "\r\n",
                "\r\n",
                "dict = {\r\n",
                "    \"Website\" : links_list,\r\n",
                "    \"WordPress\" : is_wordpress,\r\n",
                "    #\"Keyword\" : search_for,\r\n",
                "}\r\n",
                "\r\n",
                "df = pd.DataFrame(dict)\r\n",
                "df = df[~df['Website'].isin(['https://allegro.pl', 'https://www.olx.pl', 'https://pl.wikipedia.org', 'https://www.facebook.com', 'https://maps.google.com',\r\n",
                "'https://www.yelp.com', 'https://www.indeed.com'])]\r\n",
                "df = df.drop_duplicates(subset=['Website'])\r\n",
                "df_wp = df[df['WordPress'].isin([True])]\r\n",
                "df_non_wp = df[df['WordPress'].isin([False])]\r\n",
                "df_wp\r\n",
                "now = datetime.now().strftime(\"%d-%m-%Y_%H-%M\")\r\n",
                "# df_wp.to_excel(rf\"C:\\Projekty\\Coding\\wp_security_report\\leads\\output\\g_scrap_wp_{now}.xlsx\", index=False)\r\n",
                "# df_non_wp.to_excel(rf\"C:\\Projekty\\Coding\\wp_security_report\\leads\\output_non_wp\\g_scrap_non_wp_{now}.xlsx\", index=False)\r\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Clovis 1 / 10 | Progress:  10.00 %\n",
                        "Clovis 2 / 10 | Progress:  20.00 %\n",
                        "2: 'NoneType' object is not subscriptable\n",
                        "Clovis 3 / 10 | Progress:  30.00 %\n",
                        "2: 'NoneType' object is not subscriptable\n",
                        "Clovis 4 / 10 | Progress:  40.00 %\n",
                        "2: 'NoneType' object is not subscriptable\n",
                        "Clovis 5 / 10 | Progress:  50.00 %\n",
                        "2: 'NoneType' object is not subscriptable\n",
                        "Clovis 6 / 10 | Progress:  60.00 %\n",
                        "2: 'NoneType' object is not subscriptable\n",
                        "Clovis 7 / 10 | Progress:  70.00 %\n",
                        "2: 'NoneType' object is not subscriptable\n",
                        "Clovis 8 / 10 | Progress:  80.00 %\n",
                        "2: 'NoneType' object is not subscriptable\n",
                        "Clovis 9 / 10 | Progress:  90.00 %\n",
                        "2: 'NoneType' object is not subscriptable\n"
                    ]
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.13",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.13 64-bit (windows store)"
        },
        "interpreter": {
            "hash": "8065859a666a0a35ff186cc2b2b1d9fd6755a5e815cd2155f24ca932ca25e802"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}